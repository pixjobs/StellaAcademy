# infra/docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports: ["11434:11434"]
    # GPU access in WSL: requires NVIDIA Container Toolkit installed in WSL
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    # Alternative syntax if your Docker supports it:
    # gpus: all
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/ps"]
      interval: 10s
      timeout: 3s
      retries: 10
    restart: unless-stopped

  # one-time model pull helper; runs then exits
  ollama-init:
    image: curlimages/curl:8.8.0
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint:
      - sh
      - -lc
      - >
        curl -sS -X POST http://ollama:11434/api/pull
        -d '{"name":"gpt-oss:20b"}' || true

  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    container_name: stella-frontend
    environment:
      # Keep these minimal so you can run offline
      NEXT_PUBLIC_APP_NAME: "Stella Academy"
      NEXT_PUBLIC_OLLAMA_MODEL: "gpt-oss:20b"
    ports: ["3000:3000"]
    depends_on:
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    restart: unless-stopped

  # Optional math checker (comment out if not needed)
  backend:
    build:
      context: ../backend
    container_name: stella-backend
    ports: ["8001:8001"]
    restart: unless-stopped

volumes:
  ollama:
